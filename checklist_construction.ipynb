{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle \n",
    "from src.gpt_model import Model\n",
    "import ast\n",
    "\n",
    "DIMENSION = \"coherence\"\n",
    "coherence_rubric = 'Coherence - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic.\"'\n",
    "consistency_rubric = \"Consistency - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts. \"\n",
    "fluency_rubric = 'Fluency - the quality of individual sentences. Drawing again from the DUC quality guidelines, sentences in the summary \"should have no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read.\"'\n",
    "relevance_rubric = \"Relevance - selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information.\"\n",
    "\n",
    "DIMENSION_RUBRIC = {\n",
    "    \"coherence\": coherence_rubric,\n",
    "    \"consistency\": consistency_rubric, \n",
    "    \"fluency\": fluency_rubric,\n",
    "    \"relevance\": relevance_rubric\n",
    "}\n",
    "\n",
    "with open(f\"./data/{DIMENSION}/human_llm_attributes.txt\", \"r\") as file:\n",
    "    attributes_list = file.read().splitlines()\n",
    "attributes = \"\\n\".join(attributes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Does the summary start with an introduction, followed by the main body, and conclude with a summary?\n",
      "- Is the summary organized in a way that makes it easy to follow and understand?\n",
      "- Does the summary build upon each sentence to form a coherent body of information?\n",
      "- Does each sentence in the summary logically follow the previous one?\n",
      "- Are transitions between sentences and ideas effectively used in the summary?\n",
      "- Does the summary maintain a logical sequence of ideas throughout?\n",
      "- Are pronouns used in the summary clearly linked to their antecedents?\n",
      "- Are references in the summary consistent and coherent?\n",
      "- Does the summary avoid ambiguity in the use of pronouns and references?\n",
      "- Does the summary provide a clear and concise overview of the original document?\n",
      "- Does the summary effectively convey the essential information without unnecessary details?\n",
      "- Is the language used in the summary clear and free from confusion?\n",
      "- Does the summary maintain a consistent tone and style throughout?\n",
      "- Does the summary avoid redundancy or repetition that could disrupt the flow?\n",
      "- Is the information presented in the summary consistent with the information in the original document?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "checklist = pd.read_pickle(\"../InteractEval copy/data/coherence/human_llm_checklist.pkl\")\n",
    "\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4\n"
     ]
    }
   ],
   "source": [
    "with open(\"./api_keys.json\", \"r\") as file:\n",
    "    api_keys = json.load(file)\n",
    "\n",
    "OPENAI_API_KEY = api_keys[\"openai\"]\n",
    "\n",
    "gpt4 = Model(model=\"gpt-4\", temperature=0.0, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/component_extraction/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "with open(\"./prompts/checklist_construction/component_extraction/user_prompt.txt\", \"r\") as file:\n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt.format(DIMENSION, DIMENSION, DIMENSION_RUBRIC[DIMENSION])},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(attributes)}\n",
    "]\n",
    "\n",
    "gpt4_response = gpt4.ask_chatgpt(prompt_list)\n",
    "components = ast.literal_eval(gpt4_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributes Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/attributes_clustering/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "with open(\"./prompts/checklist_construction/attributes_clustering/user_prompt.txt\", \"r\") as file:\n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(components, attributes)}\n",
    "]\n",
    "\n",
    "gpt4_response = gpt4.ask_chatgpt(prompt_list)\n",
    "components_attributes_dic = eval(gpt4_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_attributes = \"\"\n",
    "for k, v in components_attributes_dic.items():\n",
    "    components_attributes += f\"{k}:\\n{v}\\n\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/question_generation/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "with open(\"./prompts/checklist_construction/question_generation/user_prompt.txt\", \"r\") as file:\n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt.format(DIMENSION)},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(DIMENSION, DIMENSION, DIMENSION_RUBRIC[DIMENSION], components_attributes)}\n",
    "]\n",
    "\n",
    "generated_key_questions = gpt4.ask_chatgpt(prompt_list)\n",
    "\n",
    "generated_key_questions = eval(generated_key_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_questions = \"\"\n",
    "for component, question in generated_key_questions.items():\n",
    "    key_questions += \"- \"+component+\": \"+question+\"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/sub_question_generation/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "with open(\"./prompts/checklist_construction/sub_question_generation/user_prompt.txt\", \"r\") as file:    \n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(DIMENSION, DIMENSION, DIMENSION, DIMENSION_RUBRIC[DIMENSION], key_questions)}\n",
    "]\n",
    "generated_sub_questions = gpt4.ask_chatgpt(prompt_list)\n",
    "generated_sub_questions = eval(generated_sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_questions = \"\"\n",
    "for component, sub_question_list in generated_sub_questions.items():\n",
    "    for sub_question in sub_question_list:\n",
    "        sub_questions += f\"- {sub_question}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/question_validation/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "with open(\"./prompts/checklist_construction/question_validation/user_prompt.txt\", \"r\") as file:    \n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(DIMENSION, DIMENSION, DIMENSION_RUBRIC[DIMENSION], DIMENSION, sub_questions)}\n",
    "]\n",
    "\n",
    "final_sub_questions = gpt4.ask_chatgpt(prompt_list)\n",
    "\n",
    "final_sub_questions_list = ast.literal_eval(final_sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = \"\"\n",
    "\n",
    "for sub_question in final_sub_questions_list:\n",
    "\n",
    "    checklist+=f\"- {sub_question}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Does the summary start with an introduction that sets the context for the following information?\n",
      "- Does each sentence in the summary contribute to the overall narrative or argument?\n",
      "- Does the summary conclude with a summary or wrap-up of the information presented?\n",
      "- Are the ideas in the summary presented in a logical order that makes it easy to follow?\n",
      "- Does each sentence in the summary logically connect to the one before it?\n",
      "- Are transitions, pronouns, and linking words used effectively to maintain a logical flow?\n",
      "- Does the summary avoid abrupt jumps between ideas or events?\n",
      "- Does the summary use clear and understandable language?\n",
      "- Does the summary accurately represent the main points of the original document?\n",
      "- Does the summary avoid including unnecessary details that distract from the main points?\n",
      "- Are the sentences in the summary clear and concise?\n",
      "- Does the summary avoid using confusing language or jargon?\n",
      "- Does the summary maintain the same context as the original document?\n",
      "- Does the summary use a consistent tone and style throughout?\n",
      "- Does the summary present a clear and logical conclusion based on the evidence and examples presented?\n",
      "- Does the summary convey all the essential information from the original document?\n",
      "- Does the summary avoid leaving out important details from the original document?\n",
      "- Does the summary provide a complete and accurate representation of the original document?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(checklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./coherence_checklist.txt\", \"w\") as file:\n",
    "    file.write(checklist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
